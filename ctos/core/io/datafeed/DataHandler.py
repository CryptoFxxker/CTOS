"""
æ•°æ®å¤„ç†å™¨æœåŠ¡ - æ”¯æŒæ•°æ®æŠ“å–å’ŒHTTP APIæœåŠ¡ä¸¤ç§æ¨¡å¼
"""
import mysql.connector
from mysql.connector import Error
from datetime import datetime, timedelta, date
import pandas as pd
import os
from tqdm import tqdm
import requests
import zipfile
import time
import random
import json
from collections import defaultdict
from mysql.connector.errors import DatabaseError
import argparse
import sys
from pathlib import Path

# å¯¼å…¥é…ç½®å’Œå·¥å…·å‡½æ•°
try:
    from ctos.drivers.okx.util import base_url, rate_price2order
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
    base_url = "https://data.binance.vision/data/spot/daily/klines"
    rate_price2order = {
        'btc': 0.01, 'eth': 0.1, 'xrp': 100, 'bnb': 0.01, 'sol': 1,
        'ada': 100, 'doge': 1000, 'trx': 1000, 'ltc': 1, 'shib': 1000000,
        'link': 1, 'dot': 1, 'om': 10, 'apt': 1, 'uni': 1, 'hbar': 100,
        'ton': 1, 'sui': 1, 'avax': 1, 'fil': 0.1, 'ip': 1, 'gala': 10,
        'sand': 10, 'trump': 0.1, 'pol': 10, 'icp': 0.01, 'cro': 10,
        'aave': 0.1, 'xlm': 100, 'bch': 0.1, 'xaut': 0.001, 'core': 1,
        'theta': 10, 'algo': 10, 'etc': 10, 'near': 10, 'hype': 0.1,
        'inj': 0.1, 'ldo': 1, 'atom': 1, 'pengu': 100, 'wld': 1,
        'render': 1, 'pepe': 10000000, 'ondo': 10, 'stx': 10, 'arb': 10,
        'jup': 10, 'bonk': 100000, 'op': 1, 'tia': 1, 'crv': 1, 'imx': 1, 'xtz': 1
    }

try:
    from ctos.core.runtime.Config import HOST_IP_1, HOST_USER, HOST_PASSWD
except ImportError:
    HOST_IP_1 = "localhost"
    HOST_USER = "user"
    HOST_PASSWD = "password"

# é…ç½®å¸¸é‡
DEFAULT_TIME_GAPS = ['1m', '15m', '30m', '1h', '4h', '1d']
STEP_SEC = {
    '1m': 60, '5m': 300, '15m': 900,
    '30m': 1800, '1h': 3600, '4h': 14400,
    '1d': 86400
}

# è·å– storage ç›®å½•è·¯å¾„ - æŒ‡å‘ ctos/core/io/storage
_STORAGE_BASE = Path(__file__).parent.parent / 'storage'
_STORAGE_BASE.mkdir(exist_ok=True)
STORAGE_PATH = _STORAGE_BASE
DATA_PATH = STORAGE_PATH / 'data'
CACHE_PATH = STORAGE_PATH / 'cache'
CACHE_FILE = CACHE_PATH / 'start_date_cache.json'

# åˆ›å»ºå¿…è¦çš„ç›®å½•
DATA_PATH.mkdir(exist_ok=True)
CACHE_PATH.mkdir(exist_ok=True)

# å…¼å®¹ä¸åŒåˆ—åçš„å­—å…¸æ˜ å°„
COLUMN_MAPPING = {
    'trade_date': 'trade_date',
    'Open': 'open',
    'High': 'high',
    'Low': 'low',
    'Close': 'close',
    'vol1': 'vol1',
    'vol': 'vol',
}


class DataHandler:
    """æ•°æ®å¤„ç†æ ¸å¿ƒç±»"""
    def __init__(self, host, database, user, password):
        self.conn = None
        try:
            self.conn = mysql.connector.connect(
                host=host,
                database=database,
                user=user,
                password=password
            )
            if self.conn.is_connected():
                print('DataHandler åˆå§‹åŒ–æˆåŠŸ')
        except Error as e:
            print(f'æ•°æ®åº“è¿æ¥å¤±è´¥: {e}')

    def create_table_if_not_exists(self, cursor, table_name):
        """åˆ›å»ºè¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰"""
        create_table_query = f"""
        CREATE TABLE IF NOT EXISTS {table_name} (
            trade_date DATETIME PRIMARY KEY,
            open DECIMAL(25, 10),
            high DECIMAL(25, 10),
            low DECIMAL(25, 10),
            close DECIMAL(25, 10),
            vol1 DECIMAL(25, 10),
            vol DECIMAL(25, 10)
        );
        """
        try:
            cursor.execute(create_table_query)
            print(f"è¡¨ {table_name} åˆ›å»ºæˆåŠŸ")
        except Error as e:
            print(f"åˆ›å»ºè¡¨ {table_name} å¤±è´¥: {e}")

    def insert_data(self, symbol, interval, data, remove_duplicates=False):
        """æ’å…¥æ•°æ®åˆ°æ•°æ®åº“"""
        table_name = f"{symbol.replace('-', '_')}_{interval}"
        try:
            if self.conn.is_connected():
                cursor = self.conn.cursor()
                query = f"""INSERT INTO {table_name}
                            (trade_date, open, high, low, close, vol1, vol)
                            VALUES (%s, %s, %s, %s, %s, %s, %s)
                            ON DUPLICATE KEY UPDATE
                            open = VALUES(open), high = VALUES(high), low = VALUES(low),
                            close = VALUES(close), vol1 = VALUES(vol1), vol = VALUES(vol);"""
                
                data['vol1'] = data['vol1'] / 1e6
                formatted_data = [
                    (
                        parse_trade_date(row['trade_date']),
                        row['open'],
                        row['high'],
                        row['low'],
                        row['close'],
                        row['vol1'],
                        row['vol']
                    )
                    for index, row in data.iterrows()
                ]

                cursor.executemany(query, formatted_data)
                self.conn.commit()
                print(cursor.rowcount, "æ¡è®°å½•å·²æ’å…¥", table_name)
                if remove_duplicates:
                    self.remove_duplicates(table_name)
            else:
                print('æ•°æ®åº“æœªè¿æ¥')
        except Error as e:
            print(f'æ’å…¥æ•°æ®å¤±è´¥: {e}')

    def remove_duplicates(self, table_name):
        """ç§»é™¤é‡å¤æ•°æ®"""
        try:
            with self.conn.cursor() as cur:
                cur.execute(f"CREATE TEMPORARY TABLE keep_dates AS "
                            f"SELECT MIN(trade_date) AS trade_date FROM {table_name} GROUP BY trade_date")

                cur.execute(f"""
                    DELETE t FROM {table_name} t
                    LEFT JOIN keep_dates k USING (trade_date)
                    WHERE k.trade_date IS NULL;
                """)
                self.conn.commit()
                cur.execute("DROP TEMPORARY TABLE keep_dates")
                print(f"å·²ç§»é™¤è¡¨ {table_name} ä¸­çš„é‡å¤æ•°æ®")
        except Error as e:
            print(f"ç§»é™¤é‡å¤æ•°æ®å¤±è´¥: {e}")

    def fetch_data(self, symbol, interval, *args):
        """
        è·å–æ•°æ®
        - ä¸€ä¸ªå‚æ•°: è·å–æœ€å X æ¡æ•°æ®
        - ä¸¤ä¸ªå‚æ•°(æ—¥æœŸå­—ç¬¦ä¸², æ•´æ•°): ä»æŒ‡å®šæ—¥æœŸå¼€å§‹/ç»“æŸè·å– X æ¡æ•°æ®
        - ä¸¤ä¸ªå‚æ•°(ä¸¤ä¸ªæ—¥æœŸå­—ç¬¦ä¸²): è·å–æŒ‡å®šæ—¥æœŸèŒƒå›´çš„æ•°æ®
        """
        table_name = f"{symbol.replace('-', '_')}_{interval}"
        safe_table_name = table_name

        if len(args) == 1 and isinstance(args[0], int):
            query = f"SELECT * FROM {safe_table_name} ORDER BY trade_date DESC LIMIT %s"
            params = (args[0],)

        elif len(args) == 2 and isinstance(args[0], str) and isinstance(args[1], int):
            if '-' in args[0]:
                query = f"""SELECT * FROM {safe_table_name}
                            WHERE trade_date >= %s
                            ORDER BY trade_date ASC
                            LIMIT %s"""
            else:
                query = f"""SELECT * FROM {safe_table_name}
                            WHERE trade_date <= %s
                            ORDER BY trade_date DESC
                            LIMIT %s"""
            params = (args[0], args[1])

        elif len(args) == 2 and isinstance(args[0], str) and isinstance(args[1], str):
            query = f"""SELECT * FROM {safe_table_name}
                        WHERE trade_date BETWEEN %s AND %s"""
            params = (args[0], args[1])
        else:
            return pd.DataFrame()

        try:
            if self.conn.is_connected():
                cursor = self.conn.cursor(dictionary=True)
                cursor.execute(query, params)
                result = cursor.fetchall()
                df = pd.DataFrame(result)
                if 'DESC' in query:
                    df = df.iloc[::-1].reset_index(drop=True)
                return df
        except Error as e:
            print(f"è·å–æ•°æ®å¤±è´¥: {e}")
            return pd.DataFrame()

    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.conn is not None and self.conn.is_connected():
            self.conn.close()
            print('æ•°æ®åº“è¿æ¥å·²å…³é—­')

    def check_missing_days(self, start_date=None, coins=None, intervals=None):
        """æ£€æŸ¥ç¼ºå¤±çš„äº¤æ˜“æ—¥"""
        if intervals is None:
            intervals = DEFAULT_TIME_GAPS
        if coins is None:
            coins = [x for x in rate_price2order.keys() if x != 'ip']

        missing_map = {}

        for cc in coins:
            if not start_date:
                start_date = find_start_date(base_url, cc.upper() + 'USDT', '1d')
            start_dt = pd.to_datetime(start_date)
            end_dt = datetime.utcnow().date() - timedelta(days=1)

            coin = cc.upper() + 'USDT'
            for interval in intervals:
                try:
                    df = self.fetch_data(
                        coin, interval,
                        start_dt.strftime("%Y-%m-%d"),
                        end_dt.strftime("%Y-%m-%d 23:59:59")
                    )
                    if df.empty:
                        exp_days = pd.date_range(start_dt, end_dt, freq='D').date
                        missing_map.setdefault(coin, {})[interval] = list(exp_days)
                        print(f"[ç©ºè¡¨] {coin}-{interval} ç¼ºå¤± {len(exp_days)} å¤©")
                        continue

                    df['trade_date'] = pd.to_datetime(df['trade_date'], unit='ms')
                    present_days = set(df['trade_date'].dt.date.unique())
                    expected_days = pd.date_range(start_dt, end_dt, freq='D').date
                    missing_days = sorted(set(expected_days) - present_days)

                    if missing_days:
                        missing_map.setdefault(coin, {})[interval] = missing_days
                        print(f"[ç¼ºå¤±] {coin}-{interval}: {len(missing_days)} å¤©")
                except Exception as e:
                    print(f"æ£€æŸ¥å¤±è´¥ {coin}-{interval}: {e}")
            start_date = None
        return missing_map


def check_data_exists(base_url, symbol, interval, date):
    """æ£€æŸ¥æ•°æ®æ˜¯å¦å­˜åœ¨"""
    date_str = date.strftime('%Y-%m-%d')
    filename = f"{symbol}-{interval}-{date_str}.zip"
    url = f"{base_url}/{symbol}/{interval}/{filename}"
    response = requests.get(url)
    return response.status_code == 200


def _load_cache():
    """åŠ è½½ç¼“å­˜"""
    if not CACHE_FILE.exists():
        return {}
    try:
        with open(CACHE_FILE, "r") as f:
            return json.load(f)
    except Exception:
        return {}


def _save_cache(cache):
    """ä¿å­˜ç¼“å­˜"""
    with open(CACHE_FILE, "w") as f:
        json.dump(cache, f, default=str, indent=2)


def find_start_date(base_url, symbol, interval, earliest_date=datetime(2015, 1, 1), latest_date=datetime.now()):
    """æŸ¥æ‰¾æ•°æ®çš„èµ·å§‹æ—¥æœŸ"""
    key = f"{symbol}_{interval}"
    cache = _load_cache()

    if key in cache:
        cached_val = datetime.fromisoformat(cache[key])
        print(f"âš¡ ç¼“å­˜å‘½ä¸­ï¼š{symbol}-{interval} -> {cached_val.date()}")
        return cached_val

    print(f"ğŸ” æ­£åœ¨æŸ¥æ‰¾ {symbol} - {interval} æœ€æ—©çš„æ•°æ®èµ·å§‹æ—¶é—´...")
    left, right, result = earliest_date, latest_date, None

    while left <= right:
        mid = left + (right - left) // 2
        exists = check_data_exists(base_url, symbol, interval, mid)
        print(f"æ£€æŸ¥ {mid.strftime('%Y-%m-%d')} : {'å­˜åœ¨âœ…' if exists else 'ä¸å­˜åœ¨âŒ'}")

        if exists:
            result = mid
            right = mid - timedelta(days=1)
        else:
            left = mid + timedelta(days=1)

    print(f"ğŸ“Œ æœ€æ—©çš„æ•°æ®èµ·å§‹æ—¶é—´æ˜¯ï¼š{result if result else 'æœªæ‰¾åˆ°'}")

    if result:
        cache[key] = result.isoformat()
        _save_cache(cache)

    return result


def download_and_process_binance_data(base_url, symbol, start_date, end_date, intervals, missing_days=None):
    """ä¸‹è½½å¹¶å¤„ç†å¸å®‰æ•°æ®"""
    if missing_days is None:
        all_days = pd.date_range(start_date.date(), end_date.date() - timedelta(days=1), freq='D').date
    else:
        all_days = sorted(missing_days)

    for interval in intervals:
        interval_dir = DATA_PATH / interval
        interval_dir.mkdir(exist_ok=True)
        
        for day in tqdm(all_days, desc=f"ä¸‹è½½ {symbol}-{interval}"):
            date_str = day.strftime('%Y-%m-%d')
            filename = f"{symbol}-{interval}-{date_str}.zip"
            csv_filename = f"{symbol}-{interval}-{date_str}.csv"
            target_csv_path = interval_dir / csv_filename
            
            IS_DOWNLOAD = False
            if not target_csv_path.exists():
                time.sleep(0.1 + random.randint(0, 20) / 20)
                url = f"{base_url}/{symbol}/{interval}/{filename}"
                response = requests.get(url)
                if response.status_code == 200:
                    zip_path = interval_dir / filename
                    with open(zip_path, 'wb') as f:
                        f.write(response.content)

                    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                        zip_ref.extractall(interval_dir)

                    extracted_file = interval_dir / csv_filename.replace('.csv', '.csv')
                    if extracted_file.exists():
                        extracted_file.rename(target_csv_path)
                    
                    zip_path.unlink()
                    IS_DOWNLOAD = True
                elif response.status_code == 404:
                    time.sleep(0.1)
                    continue
                else:
                    time.sleep(0.2)
                    print(f"ä¸‹è½½å¤±è´¥ {date_str}: çŠ¶æ€ç  {response.status_code}")
            
            if target_csv_path.exists() and IS_DOWNLOAD:
                df = pd.read_csv(target_csv_path, header=None,
                                 names=["Open time", "Open", "High", "Low", "Close", "Volume", "Close time",
                                        "Quote asset volume", "Number of trades", "Taker buy base asset volume",
                                        "Taker buy quote asset volume", "Ignore"])
                try:
                    open_time = pd.to_numeric(df['Open time'], errors='coerce')
                    if open_time.max() > 1e13:
                        open_time = open_time // 1000
                    df['trade_date'] = pd.to_datetime(open_time, unit='ms')
                    df['vol1'] = df['Quote asset volume']
                    df['vol'] = df['Volume']
                    df = df[['trade_date', 'Open', 'High', 'Low', 'Close', 'vol1', 'vol']]
                    df.columns = df.columns.str.lower()
                    df.to_csv(target_csv_path, index=False)
                except Exception as e:
                    print('\n', e, '\n', target_csv_path, '\n', df)
                    if str(e).find('Out of b') != -1:
                        break


def parse_trade_date(trade_date):
    """è§£æäº¤æ˜“æ—¥æœŸ"""
    if isinstance(trade_date, (pd.Timestamp, datetime)):
        return trade_date.strftime('%Y-%m-%d %H:%M:%S')

    if isinstance(trade_date, (int, float)):
        seconds = trade_date / 1000 if trade_date > 1e11 else trade_date
        return datetime.utcfromtimestamp(seconds).strftime('%Y-%m-%d %H:%M:%S')

    try:
        ts = pd.to_datetime(trade_date, errors='raise')
        return ts.strftime('%Y-%m-%d %H:%M:%S')
    except Exception as e:
        raise ValueError(f"æ— æ³•è§£æ trade_date={trade_date}: {e}")


def get_all_binance_data(symbol_now='ETHUSDT', missing_days=None):
    """è·å–æ‰€æœ‰å¸å®‰æ•°æ®"""
    symbol = symbol_now
    start_date = find_start_date(base_url, symbol, '1d')
    end_date = datetime.now()
    intervals = DEFAULT_TIME_GAPS
    download_and_process_binance_data(base_url, symbol, start_date, end_date, intervals, missing_days)


def read_processed_data(symbol, interval, start_date, end_date, missing_days=None):
    """è¯»å–å¤„ç†åçš„æ•°æ®"""
    start_date = pd.to_datetime(start_date).date()
    end_date = pd.to_datetime(end_date).date()

    if missing_days is None:
        dates_to_read = pd.date_range(start_date, end_date - timedelta(days=1), freq='D').date
    else:
        dates_to_read = sorted(d for d in missing_days if start_date <= d < end_date)

    interval_dir = DATA_PATH / interval
    all_data = []

    for day in dates_to_read:
        date_str = day.strftime('%Y-%m-%d')
        filename = f"{symbol}-{interval}-{date_str}.csv"
        file_path = interval_dir / filename

        if file_path.exists():
            df = pd.read_csv(file_path, parse_dates=['trade_date'])
            df.columns = df.columns.str.lower()
            all_data.append(df)
        else:
            print(f"âš ï¸  æ–‡ä»¶ç¼ºå¤±: {file_path}")

    if all_data:
        combined_df = pd.concat(all_data, ignore_index=True)
        return combined_df
    else:
        return pd.DataFrame()


def batch_insert_data(data_handler, symbol, interval, df, batch_size=1000, missing_days=None):
    """æ‰¹é‡æ’å…¥æ•°æ®"""
    if missing_days is not None and not df.empty:
        df = df[df['trade_date'].dt.date.isin(missing_days)]
        if df.empty:
            print(f"\r[{symbol}-{interval}] æ— éœ€æ’å…¥ï¼ˆç¼ºå¤±æ—¥å·²å…¨éƒ¨è¡¥é½ï¼‰", end='')
            return

    for start in tqdm(range(0, len(df), batch_size), desc=f"æ‰¹é‡æ’å…¥ {symbol}-{interval}"):
        end = start + batch_size
        batch_df = df.iloc[start:end]
        data_handler.insert_data(symbol, interval, batch_df)
        print(f"å·²æ’å…¥æ‰¹æ¬¡ {start} ~ {end - 1}")

    table_name = f"{symbol.replace('-', '_')}_{interval}"
    data_handler.remove_duplicates(table_name)


def insert_binance_data_into_mysql(data_handler, symbol_now='ETHUSDT', missing_days=None):
    """å°†å¸å®‰æ•°æ®æ’å…¥MySQL"""
    symbol = symbol_now.upper()
    start_date = find_start_date(base_url, symbol, '1d')
    end_date = datetime.now()

    for interval in tqdm(DEFAULT_TIME_GAPS, desc=f"æ’å…¥æ•°æ® {symbol}"):
        df = read_processed_data(symbol, interval, start_date, end_date, missing_days)

        if df.empty:
            print(f"[{symbol}-{interval}] æ— æ•°æ®å¯è¯»")
            continue

        batch_insert_data(
            data_handler=data_handler,
            symbol=symbol,
            interval=interval,
            df=df,
            missing_days=missing_days
        )


def export_daily_data(data_handler, base_path=None):
    """æŒ‰å¤©å¯¼å‡ºKçº¿æ•°æ®åˆ°CSVæ–‡ä»¶"""
    if base_path is None:
        base_path = STORAGE_PATH / 'exported_data'
    else:
        base_path = Path(base_path).expanduser()
    
    base_path.mkdir(exist_ok=True)
    
    time_gaps = DEFAULT_TIME_GAPS
    coins = [x for x in list(rate_price2order.keys()) if x != 'ip']

    for cc in coins:
        for interval in time_gaps:
            try:
                coin = cc.upper() + 'USDT'
                df_all = data_handler.fetch_data(coin, interval, '2017-01-01', '2025-05-03')
                if df_all.empty:
                    print(f"æ— æ•°æ®å¯å¯¼å‡º: {coin}_{interval}")
                    continue

                df_all['trade_date'] = pd.to_datetime(df_all['trade_date'])
                unique_dates = df_all['trade_date'].dt.date.unique()

                for date in unique_dates:
                    start_time = datetime.combine(date, datetime.min.time())
                    end_time = start_time + timedelta(days=1) - timedelta(seconds=1)

                    df_day = data_handler.fetch_data(
                        coin, interval,
                        start_time.strftime("%Y-%m-%d %H:%M:%S"),
                        end_time.strftime("%Y-%m-%d %H:%M:%S")
                    )
                    if df_day.empty:
                        continue

                    save_dir = base_path / interval
                    save_dir.mkdir(exist_ok=True)

                    filename = f"{coin}-{interval}-{date.strftime('%Y-%m-%d')}.csv"
                    filepath = save_dir / filename

                    if filepath.exists():
                        print(f"\r å·²å­˜åœ¨: {filepath}", end='')
                    else:
                        df_day.to_csv(filepath, index=False)
                        print(f"\rå·²ä¿å­˜: {filepath}", end='')

            except Exception as e:
                print(f"å¤„ç†å¤±è´¥ {coin}_{interval}: {str(e)}")
                continue


def check_and_repair_tables(data_handler, coins, time_gaps):
    """æ£€æŸ¥å¹¶ä¿®å¤è¡¨ä¸­çš„æ•°æ®ç¼ºå£"""
    conn = data_handler.conn
    cur = conn.cursor(dictionary=True)

    for coin in coins:
        symbol = f"{coin.upper()}USDT"
        for iv in time_gaps:
            step = STEP_SEC[iv]
            table = f"{symbol}_{iv}"

            cur.execute(f"SELECT MIN(trade_date) AS min_dt, MAX(trade_date) AS max_dt FROM {table}")
            row = cur.fetchone()
            if not row['min_dt']:
                print(f"[ç©ºè¡¨] {table} è·³è¿‡")
                continue
            t_min, t_max = row['min_dt'], row['max_dt']
            print(f"\nğŸ” {table} æ‰«æ {t_min} â†’ {t_max}")

            exist_sql = f"SELECT 1 FROM {table} WHERE trade_date = %s LIMIT 1"
            insert_sql = (
                f"INSERT INTO {table} "
                f"(trade_date, open, high, low, close, vol1, vol) "
                f"SELECT %s, open, high, low, close, vol1, vol "
                f"FROM {table} WHERE trade_date = %s LIMIT 1"
            )

            t_cur = t_min
            inserted, checked = 0, 0

            while t_cur < t_max:
                t_next = t_cur + timedelta(seconds=step)
                cur.execute(exist_sql, (t_next.strftime("%Y-%m-%d %H:%M:%S"),))
                exists = cur.fetchone()
                checked += 1
                print(f'\r {t_cur}', end='')
                if not exists:
                    cur.execute(insert_sql, (t_next.strftime("%Y-%m-%d %H:%M:%S"), t_cur.strftime("%Y-%m-%d %H:%M:%S")))
                    inserted += 1
                    print(f'\r æ£€æµ‹åˆ° {t_cur} ä¸å­˜åœ¨ï¼æ’è¡¥ä¸€æ¬¡ï¼', end='')
                    if inserted % 5000 == 0:
                        conn.commit()
                        print(f"   å·²ä¿®è¡¥ {inserted} æ¡ â€¦")

                t_cur = t_next

            conn.commit()
            print(f"âœ… {table} æ‰«æå®Œæˆï¼Œæ£€æŸ¥ {checked} æ­¥ï¼Œè¡¥ {inserted} è¡Œ")

    cur.close()
    print("\nğŸ‰ æ‰€æœ‰è¡¨ä¿®è¡¥å®Œæ¯•")


# ==================== å‘½ä»¤è¡Œå’ŒAPIæœåŠ¡éƒ¨åˆ† ====================

def run_sync_mode(args):
    """è¿è¡Œæ•°æ®åŒæ­¥æ¨¡å¼ - æ£€æŸ¥ç¼ºå¤± -> ä¸‹è½½ -> æ’å…¥"""
    print("=" * 60)
    print("æ•°æ®åŒæ­¥æ¨¡å¼ - å®Œæ•´åŒæ­¥")
    print("=" * 60)
    
    data_handler = DataHandler(args.host, args.database, args.user, args.password)
    
    if not data_handler.conn or not data_handler.conn.is_connected():
        print("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
        return
    
    coins = args.coins if args.coins else list(rate_price2order.keys())
    intervals = args.intervals if args.intervals else DEFAULT_TIME_GAPS
    
    # åŒæ­¥æ•°æ®ï¼šæ£€æŸ¥ç¼ºå¤± -> ä¸‹è½½ -> æ’å…¥
    for coin in coins:
        try:
            coin_name = coin.upper() + 'USDT'
            print(f"\nå¤„ç†å¸ç§: {coin_name}")
            
            missing_days = data_handler.check_missing_days(
                start_date=args.start_date,
                coins=[coin],
                intervals=intervals
            )
            
            if coin_name in missing_days:
                for interval in intervals:
                    if interval in missing_days[coin_name]:
                        missing_list = missing_days[coin_name][interval]
                        if missing_list:
                            print(f"  {coin_name}-{interval}: ç¼ºå¤± {len(missing_list)} å¤©")
                            get_all_binance_data(coin_name, missing_list)
                            insert_binance_data_into_mysql(data_handler, coin_name, missing_list)
        except Exception as e:
            print(f'å¤„ç† {coin} æ—¶å‡ºé”™: {e}')
    
    data_handler.close()
    print("\nâœ… æ•°æ®åŒæ­¥å®Œæˆ")


def run_download_mode(args):
    """è¿è¡Œä¸‹è½½æ¨¡å¼ - ä»…ä¸‹è½½æ•°æ®"""
    print("=" * 60)
    print("ä¸‹è½½æ¨¡å¼ - ä»…ä¸‹è½½æ•°æ®")
    print("=" * 60)
    
    coins = args.coins if args.coins else list(rate_price2order.keys())
    
    for coin in coins:
        coin_name = coin.upper() + 'USDT'
        print(f"\nä¸‹è½½å¸ç§: {coin_name}")
        get_all_binance_data(coin_name, args.missing_days)
    
    print("\nâœ… æ•°æ®ä¸‹è½½å®Œæˆ")


def run_insert_mode(args):
    """è¿è¡Œæ’å…¥æ¨¡å¼ - ä»…æ’å…¥æ•°æ®"""
    print("=" * 60)
    print("æ’å…¥æ¨¡å¼ - ä»…æ’å…¥æ•°æ®")
    print("=" * 60)
    
    data_handler = DataHandler(args.host, args.database, args.user, args.password)
    
    if not data_handler.conn or not data_handler.conn.is_connected():
        print("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
        return
    
    coins = args.coins if args.coins else list(rate_price2order.keys())
    
    for coin in coins:
        coin_name = coin.upper() + 'USDT'
        print(f"\næ’å…¥å¸ç§: {coin_name}")
        insert_binance_data_into_mysql(data_handler, coin_name, args.missing_days)
    
    data_handler.close()
    print("\nâœ… æ•°æ®æ’å…¥å®Œæˆ")


def run_repair_mode(args):
    """è¿è¡Œä¿®å¤æ¨¡å¼ - ä¿®å¤è¡¨æ•°æ®"""
    print("=" * 60)
    print("ä¿®å¤æ¨¡å¼ - ä¿®å¤è¡¨æ•°æ®")
    print("=" * 60)
    
    data_handler = DataHandler(args.host, args.database, args.user, args.password)
    
    if not data_handler.conn or not data_handler.conn.is_connected():
        print("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
        return
    
    coins = args.coins if args.coins else list(rate_price2order.keys())
    intervals = args.intervals if args.intervals else DEFAULT_TIME_GAPS
    
    check_and_repair_tables(data_handler, coins, intervals)
    
    data_handler.close()
    print("\nâœ… è¡¨ä¿®å¤å®Œæˆ")


def run_export_mode(args):
    """è¿è¡Œå¯¼å‡ºæ¨¡å¼ - å¯¼å‡ºæ•°æ®"""
    print("=" * 60)
    print("å¯¼å‡ºæ¨¡å¼ - å¯¼å‡ºæ•°æ®åˆ°CSV")
    print("=" * 60)
    
    data_handler = DataHandler(args.host, args.database, args.user, args.password)
    
    if not data_handler.conn or not data_handler.conn.is_connected():
        print("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
        return
    
    export_daily_data(data_handler, args.export_path)
    
    data_handler.close()
    print("\nâœ… æ•°æ®å¯¼å‡ºå®Œæˆ")


def run_server_mode(args):
    """è¿è¡ŒHTTP APIæœåŠ¡æ¨¡å¼"""
    try:
        from flask import Flask, jsonify, request
    except ImportError:
        print("âŒ éœ€è¦å®‰è£… Flask: pip install flask")
        return
    
    print("=" * 60)
    print("HTTP API æœåŠ¡æ¨¡å¼")
    print("=" * 60)
    
    app = Flask(__name__)
    data_handler = DataHandler(args.host, args.database, args.user, args.password)
    
    if not data_handler.conn or not data_handler.conn.is_connected():
        print("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œæ— æ³•å¯åŠ¨æœåŠ¡")
        return
    
    @app.route('/health', methods=['GET'])
    def health():
        """å¥åº·æ£€æŸ¥"""
        return jsonify({'status': 'ok', 'service': 'DataHandler API'})
    
    @app.route('/api/data', methods=['GET'])
    def get_data():
        """è·å–Kçº¿æ•°æ®"""
        try:
            symbol = request.args.get('symbol', 'ETHUSDT')
            interval = request.args.get('interval', '1d')
            
            # æ”¯æŒå¤šç§æŸ¥è¯¢æ–¹å¼
            limit = request.args.get('limit', type=int)
            start_date = request.args.get('start_date')
            end_date = request.args.get('end_date')
            
            if limit:
                df = data_handler.fetch_data(symbol, interval, limit)
            elif start_date and end_date:
                df = data_handler.fetch_data(symbol, interval, start_date, end_date)
            elif start_date:
                limit = request.args.get('limit', 100, type=int)
                df = data_handler.fetch_data(symbol, interval, start_date, limit)
            else:
                df = data_handler.fetch_data(symbol, interval, 100)
            
            if df.empty:
                return jsonify({'error': 'No data found'}), 404
            
            # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            result = df.to_dict('records')
            return jsonify({
                'symbol': symbol,
                'interval': interval,
                'count': len(result),
                'data': result
            })
        except Exception as e:
            return jsonify({'error': str(e)}), 500
    
    @app.route('/api/missing', methods=['GET'])
    def check_missing():
        """æ£€æŸ¥ç¼ºå¤±æ•°æ®"""
        try:
            coins = request.args.getlist('coins') or None
            intervals = request.args.getlist('intervals') or None
            start_date = request.args.get('start_date')
            
            missing = data_handler.check_missing_days(
                start_date=start_date,
                coins=coins,
                intervals=intervals
            )
            
            return jsonify({
                'missing_days': missing,
                'summary': {
                    coin: {iv: len(days) for iv, days in intervals.items()}
                    for coin, intervals in missing.items()
                }
            })
        except Exception as e:
            return jsonify({'error': str(e)}), 500
    
    @app.route('/api/symbols', methods=['GET'])
    def get_symbols():
        """è·å–æ”¯æŒçš„äº¤æ˜“å¯¹åˆ—è¡¨"""
        return jsonify({
            'symbols': [f"{k.upper()}USDT" for k in rate_price2order.keys() if k != 'ip'],
            'intervals': DEFAULT_TIME_GAPS
        })
    
    host = args.server_host or '0.0.0.0'
    port = args.server_port or 5000
    
    print(f"ğŸš€ æœåŠ¡å¯åŠ¨åœ¨ http://{host}:{port}")
    print(f"ğŸ“š API æ–‡æ¡£:")
    print(f"   GET /health - å¥åº·æ£€æŸ¥")
    print(f"   GET /api/data?symbol=ETHUSDT&interval=1d&limit=100 - è·å–æ•°æ®")
    print(f"   GET /api/data?symbol=ETHUSDT&interval=1d&start_date=2024-01-01&end_date=2024-01-31 - è·å–æ—¥æœŸèŒƒå›´æ•°æ®")
    print(f"   GET /api/missing?coins=btc&coins=eth&intervals=1d - æ£€æŸ¥ç¼ºå¤±æ•°æ®")
    print(f"   GET /api/symbols - è·å–æ”¯æŒçš„äº¤æ˜“å¯¹")
    
    app.run(host=host, port=port, debug=args.debug)


def main():
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(
        description='æ•°æ®å¤„ç†å™¨æœåŠ¡ - æ”¯æŒæ•°æ®åŒæ­¥å’ŒHTTP APIæœåŠ¡',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # æ•°æ®åŒæ­¥æ¨¡å¼ - è‡ªåŠ¨æ£€æŸ¥å¹¶åŒæ­¥æ•°æ®
  python DataHandler.py sync --coins btc eth --intervals 1d
  
  # ä»…ä¸‹è½½æ•°æ®
  python DataHandler.py download --coins btc
  
  # ä»…æ’å…¥æ•°æ®
  python DataHandler.py insert --coins btc
  
  # ä¿®å¤è¡¨æ•°æ®
  python DataHandler.py repair --coins btc eth
  
  # å¯¼å‡ºæ•°æ®
  python DataHandler.py export --export-path ~/data/export
  
  # å¯åŠ¨HTTP APIæœåŠ¡
  python DataHandler.py server --host 0.0.0.0 --port 5000
        """
    )
    
    # æ·»åŠ å­å‘½ä»¤
    subparsers = parser.add_subparsers(dest='mode', help='è¿è¡Œæ¨¡å¼')
    
    # å…±äº«çš„æ•°æ®åº“å‚æ•°
    def add_db_args(subparser):
        subparser.add_argument('--host', default=HOST_IP_1, help='æ•°æ®åº“ä¸»æœº')
        subparser.add_argument('--database', default='TradingData', help='æ•°æ®åº“å')
        subparser.add_argument('--user', default=HOST_USER, help='æ•°æ®åº“ç”¨æˆ·')
        subparser.add_argument('--password', default=HOST_PASSWD, help='æ•°æ®åº“å¯†ç ')
    
    # åŒæ­¥æ¨¡å¼å­å‘½ä»¤
    sync_parser = subparsers.add_parser('sync', help='æ•°æ®åŒæ­¥æ¨¡å¼ï¼ˆæ£€æŸ¥ç¼ºå¤± -> ä¸‹è½½ -> æ’å…¥ï¼‰')
    add_db_args(sync_parser)
    sync_parser.add_argument('--coins', nargs='+', help='å¸ç§åˆ—è¡¨ï¼Œå¦‚: btc eth xrp')
    sync_parser.add_argument('--intervals', nargs='+', default=DEFAULT_TIME_GAPS,
                            help='æ—¶é—´å‘¨æœŸåˆ—è¡¨ï¼Œå¦‚: 1m 15m 1d')
    sync_parser.add_argument('--start-date', dest='start_date', help='èµ·å§‹æ—¥æœŸ YYYY-MM-DD')
    
    # ä¸‹è½½æ¨¡å¼å­å‘½ä»¤
    download_parser = subparsers.add_parser('download', help='ä»…ä¸‹è½½æ•°æ®')
    download_parser.add_argument('--coins', nargs='+', help='å¸ç§åˆ—è¡¨ï¼Œå¦‚: btc eth xrp')
    download_parser.add_argument('--missing-days', nargs='+', help='ç¼ºå¤±æ—¥æœŸåˆ—è¡¨')
    
    # æ’å…¥æ¨¡å¼å­å‘½ä»¤
    insert_parser = subparsers.add_parser('insert', help='ä»…æ’å…¥æ•°æ®åˆ°æ•°æ®åº“')
    add_db_args(insert_parser)
    insert_parser.add_argument('--coins', nargs='+', help='å¸ç§åˆ—è¡¨ï¼Œå¦‚: btc eth xrp')
    insert_parser.add_argument('--missing-days', nargs='+', help='ç¼ºå¤±æ—¥æœŸåˆ—è¡¨')
    
    # ä¿®å¤æ¨¡å¼å­å‘½ä»¤
    repair_parser = subparsers.add_parser('repair', help='ä¿®å¤è¡¨æ•°æ®ï¼ˆè¡¥ç¼ºï¼‰')
    add_db_args(repair_parser)
    repair_parser.add_argument('--coins', nargs='+', help='å¸ç§åˆ—è¡¨ï¼Œå¦‚: btc eth xrp')
    repair_parser.add_argument('--intervals', nargs='+', default=DEFAULT_TIME_GAPS,
                               help='æ—¶é—´å‘¨æœŸåˆ—è¡¨ï¼Œå¦‚: 1m 15m 1d')
    
    # å¯¼å‡ºæ¨¡å¼å­å‘½ä»¤
    export_parser = subparsers.add_parser('export', help='å¯¼å‡ºæ•°æ®åˆ°CSV')
    add_db_args(export_parser)
    export_parser.add_argument('--export-path', help='å¯¼å‡ºè·¯å¾„')
    
    # æœåŠ¡å™¨æ¨¡å¼å­å‘½ä»¤
    server_parser = subparsers.add_parser('server', help='HTTP API æœåŠ¡æ¨¡å¼')
    server_parser.add_argument('--host', default='0.0.0.0', dest='server_host',
                              help='æœåŠ¡ç›‘å¬åœ°å€ (é»˜è®¤: 0.0.0.0)')
    server_parser.add_argument('--port', type=int, default=5000, dest='server_port',
                              help='æœåŠ¡ç«¯å£ (é»˜è®¤: 5000)')
    server_parser.add_argument('--db-host', default=HOST_IP_1, dest='host',
                              help='æ•°æ®åº“ä¸»æœº')
    server_parser.add_argument('--db-database', default='TradingData', dest='database',
                              help='æ•°æ®åº“å')
    server_parser.add_argument('--db-user', default=HOST_USER, dest='user',
                              help='æ•°æ®åº“ç”¨æˆ·')
    server_parser.add_argument('--db-password', default=HOST_PASSWD, dest='password',
                              help='æ•°æ®åº“å¯†ç ')
    server_parser.add_argument('--debug', action='store_true', help='å¼€å¯è°ƒè¯•æ¨¡å¼')
    
    args = parser.parse_args()
    
    if not args.mode:
        parser.print_help()
        sys.exit(1)
    
    if args.mode == 'sync':
        run_sync_mode(args)
    elif args.mode == 'download':
        run_download_mode(args)
    elif args.mode == 'insert':
        run_insert_mode(args)
    elif args.mode == 'repair':
        run_repair_mode(args)
    elif args.mode == 'export':
        run_export_mode(args)
    elif args.mode == 'server':
        run_server_mode(args)
    else:
        parser.print_help()
        sys.exit(1)


if __name__ == '__main__':
    main()
